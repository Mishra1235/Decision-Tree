{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree | Assignment"
      ],
      "metadata": {
        "id": "faDvLFJ4JNeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "### **Definition**:-  \n",
        "A **Decision Tree** is a supervised machine learning algorithm that uses a tree-like structure to make decisions.  \n",
        "- The **Root Node** represents the entire dataset.  \n",
        "- **Internal Nodes** represent tests or conditions on features.  \n",
        "- **Branches** represent outcomes of those tests.  \n",
        "- **Leaf Nodes** represent the final predicted class labels.  \n",
        "\n",
        "### **How it works in Classification**:-  \n",
        "In the context of classification, a Decision Tree follows these steps:  \n",
        "\n",
        "1. **Feature Selection**  \n",
        "   - At each step, the algorithm chooses the feature that best splits the data into distinct classes.  \n",
        "   - This selection is based on metrics such as:  \n",
        "     - **Information Gain** (using Entropy)  \n",
        "     - **Gini Impurity**  \n",
        "\n",
        "2. **Splitting the Dataset**  \n",
        "   - The dataset is divided into subsets based on the chosen feature’s values.  \n",
        "   - Each subset moves to a new branch of the tree.  \n",
        "\n",
        "3. **Recursive Partitioning**  \n",
        "   - The process of selecting features and splitting is repeated recursively for each subset.  \n",
        "   - This continues until:  \n",
        "     - All instances in a node belong to the same class, or  \n",
        "     - No further meaningful split can be made.  \n",
        "\n",
        "4. **Leaf Node Prediction**  \n",
        "   - Once a leaf node is reached, it represents the final predicted class label.  \n",
        "   - For any new instance, the model traces the path from the root to a leaf node, based on its feature values, and assigns the corresponding class label.  \n",
        "\n",
        " **Example**:-  \n",
        "Consider classifying emails into *Spam* or *Not Spam*:  \n",
        "- The tree may first split on whether the subject line contains the word **“offer”**.  \n",
        "- If yes, it may further split on whether the sender is in the recipient’s contact list.  \n",
        "- Finally, the leaf nodes represent the decision (*Spam* or *Not Spam*).\n",
        "\n",
        "### Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "### **Gini Impurity**:-  \n",
        "- **Formula:**  \n",
        "  \\[\n",
        "  Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "  \\]  \n",
        "- It measures the probability of incorrectly classifying a randomly chosen instance.  \n",
        "- Range: **0 (pure node)** to **0.5 (maximum impurity for binary classes)**.  \n",
        "\n",
        "### **Entropy**:-  \n",
        "- **Formula:**  \n",
        "  \\[\n",
        "  Entropy = - \\sum_{i=1}^{C} p_i \\cdot \\log_2(p_i)\n",
        "  \\]  \n",
        "- It measures the amount of disorder or uncertainty in the data.  \n",
        "- Range: **0 (pure node)** to **1 (maximum impurity for binary classes)**.  \n",
        "\n",
        "- **Information Gain** = Reduction in entropy after a split.  \n",
        "\n",
        "### **Impact on Splits**:-  \n",
        "- At each node, the Decision Tree algorithm evaluates possible features using Gini or Entropy.  \n",
        "- The feature that results in the **largest decrease in impurity (highest purity gain)** is selected for the split.  \n",
        "- **Gini** tends to favor splits where one class is dominant.  \n",
        "- **Entropy** focuses on reducing overall disorder.\n",
        "\n",
        " Thus, both measures guide the tree to create splits that make the resulting child nodes purer, improving classification accuracy.\n",
        "\n",
        "### Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "### **Pre-Pruning (Early Stopping)**:-  \n",
        "- **Concept:** Stops the tree from growing once a certain condition is met (e.g., maximum depth, minimum number of samples per node, minimum information gain).  \n",
        "- **Goal:** Prevents the tree from becoming too complex during training.\n",
        "\n",
        "**Advantage:-**  \n",
        "- Saves computation time and avoids overfitting by restricting unnecessary splits.  \n",
        "\n",
        "### **Post-Pruning (Pruning after Full Growth)**:-  \n",
        "- **Concept:** The tree is allowed to grow fully, and then branches that provide little to no improvement are removed.  \n",
        "- **Goal:** Simplifies the tree after training by cutting back less important branches.  \n",
        "\n",
        "**Advantage:-**  \n",
        "- Results in a simpler and more generalizable model with better performance on unseen data.  \n",
        "\n",
        " **Key Difference:-**  \n",
        "- **Pre-Pruning** controls tree growth during training.  \n",
        "- **Post-Pruning** reduces the size of a fully grown tree by removing weak splits afterward.\n",
        "\n",
        "### Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "### **Information Gain**  \n",
        "- **Definition:** Information Gain (IG) is a measure of how much **uncertainty (entropy)** in the dataset is reduced after splitting based on a particular feature.  \n",
        "- **Formula:**  \n",
        "  \\[\n",
        "  IG(D, A) = Entropy(D) - \\sum_{v \\in Values(A)} \\frac{|D_v|}{|D|} \\cdot Entropy(D_v)\n",
        "  \\]  \n",
        "  where:  \n",
        "  - \\(D\\) = dataset  \n",
        "  - \\(A\\) = feature used for splitting  \n",
        "  - \\(D_v\\) = subset of \\(D\\) where feature \\(A\\) takes value \\(v\\)  \n",
        "\n",
        "### **Importance for Choosing the Best Split**  \n",
        "1. At each node, the Decision Tree algorithm calculates Information Gain for all features.  \n",
        "2. The feature with the **highest Information Gain** is chosen, since it provides the most effective separation of classes.  \n",
        "3. High IG → maximum reduction in impurity → more **pure child nodes**.  \n",
        "4. This ensures the tree learns meaningful patterns and improves classification accuracy.  \n",
        "\n",
        " **Example:-**  \n",
        "If splitting student data by \"Study Hours\" reduces entropy more than splitting by \"Attendance\", then \"Study Hours\" will be chosen as the splitting feature because it gives higher Information Gain.\n",
        "\n",
        "### Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "### **Real-World Applications**:-\n",
        "\n",
        "1. **Classification Tasks**  \n",
        "   - **Iris Dataset**: Predicting the species of a flower based on features like petal and sepal length/width.  \n",
        "     ```python\n",
        "     from sklearn.datasets import load_iris\n",
        "     iris = load_iris()\n",
        "     X, y = iris.data, iris.target\n",
        "     ```  \n",
        "   - **Medical Diagnosis**: Predicting diseases based on patient symptoms and test results.  \n",
        "   - **Customer Segmentation**: Classifying customers based on behavior or demographics.\n",
        "\n",
        "\n",
        "2. **Regression Tasks**  \n",
        "   - **Boston Housing Dataset**: Predicting house prices based on features like number of rooms, location, etc.  \n",
        "     ```python\n",
        "     from sklearn.datasets import load_boston\n",
        "     boston = load_boston()\n",
        "     X, y = boston.data, boston.target\n",
        "     ```  \n",
        "   - **Sales Forecasting**: Predicting sales revenue from historical data.  \n",
        "   - **Risk Assessment**: Estimating financial risk scores in banking.  \n",
        "\n",
        "### **Advantages of Decision Trees**:-  \n",
        "- **Easy to Interpret:** Graphical tree structure is intuitive.  \n",
        "- **Handles Both Data Types:** Can manage categorical and numerical features.  \n",
        "- **Non-Linear Relationships:** Captures complex patterns without requiring data scaling.  \n",
        "- **Requires Minimal Data Preparation:** No need for normalization or one-hot encoding (in some cases).\n",
        "\n",
        "### **Limitations of Decision Trees**:-  \n",
        "- **Overfitting:** Deep trees can memorize training data, reducing generalization.  \n",
        "- **High Variance:** Small changes in data can lead to very different trees.  \n",
        "- **Bias Towards Dominant Features:** Features with more levels may dominate splits.  \n",
        "- **Limited Predictive Accuracy:** Alone, may not perform as well as ensemble methods like Random Forest or Gradient Boosting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lPnXmPGrPLzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Rks7igjzZnUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Decision Tree Classifier on Iris Dataset\n",
        "\n",
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Decision Tree Classifier using Gini criterion\n",
        "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate accuracy\n",
        "y_pred = dt_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print accuracy and feature importances\n",
        "print(f\"Decision Tree Accuracy: {accuracy:.2f}\")\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, dt_model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRERj6LUJE-R",
        "outputId": "171e5347-90c5-45bb-fb5e-cf9c9efa272a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.00\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.000\n",
            "sepal width (cm): 0.019\n",
            "petal length (cm): 0.893\n",
            "petal width (cm): 0.088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "zNaWamtXpjgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Compare Decision Tree with max_depth=3 vs fully-grown tree on Iris Dataset\n",
        "\n",
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Decision Tree with max_depth=3\n",
        "dt_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_limited.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Train fully-grown Decision Tree\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions\n",
        "y_pred_limited = dt_limited.predict(X_test)\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate and print accuracies\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy of Fully-Grown Decision Tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opL-nwUTp8pS",
        "outputId": "1297c678-a098-4164-e44a-8e56810599a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.00\n",
            "Accuracy of Fully-Grown Decision Tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "30xvXMw1qVmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Decision Tree Regressor on California Housing Dataset\n",
        "\n",
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 2: Load the California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X, y = california.data, california.target\n",
        "feature_names = california.feature_names\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = dt_regressor.predict(X_test)\n",
        "\n",
        "# Step 6: Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print MSE and feature importances\n",
        "print(f\"Decision Tree Regressor MSE: {mse:.3f}\")\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, dt_regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZMYP7igqjR7",
        "outputId": "d56a42c1-5445-4c78-cd6a-f0c946bc9f07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor MSE: 0.528\n",
            "Feature Importances:\n",
            "MedInc: 0.523\n",
            "HouseAge: 0.052\n",
            "AveRooms: 0.049\n",
            "AveBedrms: 0.025\n",
            "Population: 0.032\n",
            "AveOccup: 0.139\n",
            "Latitude: 0.090\n",
            "Longitude: 0.089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "jhBJooKMq6s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Hyperparameter Tuning for Decision Tree using GridSearchCV on Iris Dataset\n",
        "\n",
        "# Step 1: Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Step 3: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Step 5: Initialize GridSearchCV with Decision Tree Classifier\n",
        "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy')\n",
        "\n",
        "# Step 6: Fit GridSearchCV to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Retrieve the best parameters and evaluate model accuracy\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 8: Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Accuracy of tuned Decision Tree: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCzb51VKrLWj",
        "outputId": "787615dd-557d-4e6c-8abb-bb2b7e74a4b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy of tuned Decision Tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "### **Step 1: Handle Missing Values**\n",
        "- **Identify missing values** in both numerical and categorical columns (`df.isnull().sum()`).  \n",
        "- **Imputation Methods:**  \n",
        "  - **Numerical Features:** Fill missing values with **mean, median**, or use advanced methods like KNN imputer.  \n",
        "  - **Categorical Features:** Fill missing values with **mode** or create a separate category like \"Unknown\".  \n",
        "- **Why:** Decision Trees cannot handle missing values directly; proper imputation ensures the model can learn effectively.\n",
        "\n",
        "### **Step 2: Encode Categorical Features**\n",
        "- **Identify categorical columns**.  \n",
        "- **Encoding Options:**  \n",
        "  - **One-Hot Encoding:** For nominal categories without order.  \n",
        "  - **Label Encoding:** For ordinal categories with intrinsic order.  \n",
        "- **Why:** Decision Trees require numerical input; encoding converts categorical data to numeric form while preserving information.\n",
        "\n",
        "### **Step 3: Train a Decision Tree Model**\n",
        "- **Split dataset** into training and testing sets (e.g., 70:30).  \n",
        "- **Initialize Decision Tree Classifier**:  \n",
        "  ```python\n",
        "  from sklearn.tree import DecisionTreeClassifier\n",
        "  dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "  dt_model.fit(X_train, y_train)\n",
        "\n",
        "### **Step 4: Tune Hyperparameters**\n",
        "- Use **GridSearchCV** or **RandomizedSearchCV** to find the best parameters:  \n",
        "  - `max_depth` → controls tree complexity  \n",
        "  - `min_samples_split` → prevents overfitting  \n",
        "  - `min_samples_leaf` → ensures minimum samples at leaf nodes  \n",
        "- Example:  \n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'max_depth':[3,5,7,None], 'min_samples_split':[2,5,10]}\n",
        "grid_search = GridSearchCV(dt_model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "### **Step 5: Evaluate Model Performance**\n",
        "- Use appropriate classification metrics:  \n",
        "  - **Accuracy**: Measures overall correctness  \n",
        "  - **Precision, Recall, F1-score**: Crucial for identifying true positive cases  \n",
        "  - **ROC-AUC**: Evaluates model’s ability to distinguish between diseased and non-diseased  \n",
        "- Perform **cross-validation** to ensure the model generalizes well to unseen data.\n",
        "\n",
        "### **Step 6: Business Value in Real-World**\n",
        "- **Early Detection:** Identify high-risk patients before severe symptoms develop.  \n",
        "- **Resource Allocation:** Focus medical tests and interventions on high-risk patients.  \n",
        "- **Decision Support:** Provide doctors with data-driven insights to reduce diagnostic errors.  \n",
        "- **Cost Efficiency:** Optimize healthcare resources and reduce unnecessary procedures.\n",
        "\n",
        " Summary:-\n",
        "By following a systematic approach — handling missing values, encoding categorical features, training and tuning a Decision Tree, and evaluating performance — the model can provide actionable insights in healthcare, improving patient outcomes and operational efficiency.\n",
        "\n"
      ],
      "metadata": {
        "id": "1FIHTSSCrhRD"
      }
    }
  ]
}